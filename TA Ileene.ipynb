{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1nHvFWpAw5_"
      },
      "outputs": [],
      "source": [
        "# Install Kaggle Hub library if not installed\n",
        "!pip install kagglehub\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version of DocUNet Dataset\n",
        "path_docunet = kagglehub.dataset_download(\"minhbithun/docunet-dataset\")\n",
        "print(\"Path to DocUNet dataset files:\", path_docunet)\n",
        "\n",
        "# Download latest version of SROIE Dataset v2\n",
        "path_sroie = kagglehub.dataset_download(\"urbikn/sroie-datasetv2\")\n",
        "print(\"Path to SROIE dataset files:\", path_sroie)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "# Fungsi untuk memuat dan memproses gambar\n",
        "def load_image(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, (256, 256))  # Ukuran standar\n",
        "    img = np.expand_dims(img, axis=-1)  # Menambah dimensi channel\n",
        "    img = img.astype(\"float32\") / 255.0  # Normalisasi\n",
        "    return img\n",
        "\n",
        "# Muat dataset DocUNet dan SROIE\n",
        "docunet_images = [load_image(os.path.join(path_docunet, img)) for img in os.listdir(path_docunet)]\n",
        "sroie_images = [load_image(os.path.join(path_sroie, img)) for img in os.listdir(path_sroie)]\n",
        "\n",
        "# Gabungkan kedua dataset\n",
        "images = np.array(docunet_images + sroie_images)\n",
        "labels = np.array([0] * len(docunet_images) + [1] * len(sroie_images))  # Misalnya 0 untuk DocUNet, 1 untuk SROIE\n",
        "\n",
        "# Split data menjadi train dan test\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "XINsZK1C7Bo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fungsi untuk memuat dan memproses gambar\n",
        "def load_image(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Membaca gambar dalam grayscale\n",
        "    img = cv2.resize(img, (512, 512))  # Resize ke 512x512\n",
        "    img = img.astype(\"float32\") / 255.0  # Normalisasi ke rentang [0, 1]\n",
        "    return img\n",
        "\n",
        "# Fungsi untuk Image Enhancement\n",
        "def enhance_image(img):\n",
        "    # Contrast Limited Adaptive Histogram Equalization (CLAHE)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    img_clahe = clahe.apply((img * 255).astype(np.uint8)) / 255.0  # Klahe dan normalisasi kembali\n",
        "\n",
        "    # Denoising (Mengurangi noise)\n",
        "    img_denoised = cv2.fastNlMeansDenoising(img_clahe.astype(np.uint8), None, 10, 7, 21)\n",
        "\n",
        "    # Edge detection menggunakan Canny\n",
        "    img_edges = cv2.Canny(img_denoised.astype(np.uint8), threshold1=100, threshold2=200)\n",
        "\n",
        "    # Binarization (Thresholding) untuk teks\n",
        "    _, img_binarized = cv2.threshold(img_denoised, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    return img_binarized\n",
        "\n",
        "# Fungsi untuk Augmentasi Data\n",
        "def augment_data(image):\n",
        "    # Mempersiapkan objek ImageDataGenerator untuk augmentasi\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=30,   # Rotasi gambar hingga 30 derajat\n",
        "        width_shift_range=0.1,  # Translasi horizontal\n",
        "        height_shift_range=0.1, # Translasi vertikal\n",
        "        shear_range=0.2,     # Distorsi shear\n",
        "        zoom_range=0.2,      # Scaling\n",
        "        horizontal_flip=True,  # Flipping horizontal\n",
        "        fill_mode='nearest'   # Mengisi area yang hilang dengan pixel terdekat\n",
        "    )\n",
        "\n",
        "    # Augmentasi gambar\n",
        "    augmented_image = datagen.random_transform(image)\n",
        "    return augmented_image\n",
        "\n",
        "# Fungsi untuk memisahkan dataset menjadi training, validation, dan testing\n",
        "def split_dataset(images, test_size=0.3):\n",
        "    # Membagi dataset menjadi 70% pelatihan, 15% validasi, 15% pengujian\n",
        "    X_train, X_temp = train_test_split(images, test_size=test_size, random_state=42)\n",
        "    X_val, X_test = train_test_split(X_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    return X_train, X_val, X_test\n",
        "\n",
        "# Path untuk gambar (ganti dengan path sebenarnya)\n",
        "path_docunet = 'path_to_docunet_images'  # Ganti dengan path dataset DocUNet\n",
        "path_sroie = 'path_to_sroie_images'      # Ganti dengan path dataset SROIE\n",
        "\n",
        "# Memuat semua gambar dari kedua dataset\n",
        "docunet_images = [load_image(os.path.join(path_docunet, img)) for img in os.listdir(path_docunet)]\n",
        "sroie_images = [load_image(os.path.join(path_sroie, img)) for img in os.listdir(path_sroie)]\n",
        "\n",
        "# Gabungkan gambar dari kedua dataset\n",
        "images = np.array(docunet_images + sroie_images)\n",
        "\n",
        "# Enhancing gambar (CLAHE, denoising, edge detection, binarization)\n",
        "enhanced_images = np.array([enhance_image(img) for img in images])\n",
        "\n",
        "# Augmentasi data untuk setiap gambar\n",
        "augmented_images = np.array([augment_data(img) for img in enhanced_images])\n",
        "\n",
        "# Pisahkan dataset menjadi training, validation, dan testing\n",
        "X_train, X_val, X_test = split_dataset(augmented_images, test_size=0.3)\n",
        "\n",
        "# Tampilkan beberapa contoh gambar hasil preprocessing\n",
        "for i in range(3):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Gambar asli\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(images[i], cmap='gray')\n",
        "    plt.title(\"Original Image\")\n",
        "\n",
        "    # Gambar setelah enhancement\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(enhanced_images[i], cmap='gray')\n",
        "    plt.title(\"Enhanced Image\")\n",
        "\n",
        "    # Gambar setelah augmentasi\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(augmented_images[i], cmap='gray')\n",
        "    plt.title(\"Augmented Image\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Print size of datasets\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n"
      ],
      "metadata": {
        "id": "iQjffvqK5Yj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Mechanism Layer\n",
        "class AttentionLayer(layers.Layer):\n",
        "    def __init__(self, channels):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention = layers.Conv2D(channels, (1, 1), activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attention_weights = self.attention(inputs)\n",
        "        return inputs * attention_weights\n",
        "\n",
        "# Residual Block\n",
        "def residual_block(x, filters):\n",
        "    res = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
        "    res = layers.ReLU()(res)\n",
        "    res = layers.Conv2D(filters, (3, 3), padding='same')(res)\n",
        "    return layers.add([x, res])\n",
        "\n",
        "# Model DocUNet\n",
        "def create_docunet_model(input_shape=(256, 256, 1)):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder: Convolutional layers\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = residual_block(x, 64)\n",
        "\n",
        "    # Attention Mechanism Layer\n",
        "    x = AttentionLayer(64)(x)\n",
        "\n",
        "    # Deeper Convolutional Layers\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # Decoder: Upsampling and convolution\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
        "    x = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    model = models.Model(inputs, x)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Membuat model\n",
        "model = create_docunet_model()\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "CN78uxpx7Dg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melatih model\n",
        "history = model.fit(X_train, X_train, epochs=10, batch_size=16, validation_data=(X_test, X_test))\n"
      ],
      "metadata": {
        "id": "fS3SOtrG7HdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk mengevaluasi SSIM dan PSNR\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    ssim_score = ssim(y_true, y_pred, data_range=y_pred.max() - y_pred.min())\n",
        "    psnr_score = psnr(y_true, y_pred)\n",
        "    return ssim_score, psnr_score\n",
        "\n",
        "# Uji pada data test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluasi metrik SSIM dan PSNR untuk beberapa gambar\n",
        "ssim_scores = []\n",
        "psnr_scores = []\n",
        "for i in range(len(X_test)):\n",
        "    ssim_score, psnr_score = evaluate_metrics(X_test[i], y_pred[i])\n",
        "    ssim_scores.append(ssim_score)\n",
        "    psnr_scores.append(psnr_score)\n",
        "\n",
        "# Hitung rata-rata SSIM dan PSNR\n",
        "average_ssim = np.mean(ssim_scores)\n",
        "average_psnr = np.mean(psnr_scores)\n",
        "\n",
        "print(\"Average SSIM:\", average_ssim)\n",
        "print(\"Average PSNR:\", average_psnr)\n"
      ],
      "metadata": {
        "id": "nv6qayzF7JcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan model\n",
        "model.save('docunet_model.h5')\n",
        "\n",
        "# Menampilkan hasil rekonstruksi pada beberapa gambar\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(5):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(X_test[i].squeeze(), cmap='gray')\n",
        "    plt.title(\"Original Image\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(y_pred[i].squeeze(), cmap='gray')\n",
        "    plt.title(\"Reconstructed Image\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DnqkNrRe7K7A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}